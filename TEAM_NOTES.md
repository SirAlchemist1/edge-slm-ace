# TEAM_NOTES.md

Team handoff notes for the SLM-ACE project.

## Current Status (Person 1 - Suryodaya)

âœ… **Infrastructure Complete:**
- Core framework in place (`slm_ace/` package)
- Model loading with MPS/CUDA/CPU auto-detection
- Task registry (`--task-name` CLI flag)
- Consistent CSV schema for results
- Baseline and ACE pipeline skeletons
- Error handling and device fallback

âœ… **Verified on Mac M3 Pro:**
- Infra stable on Mac M3 Pro
- Tasks in registry: `tatqa_tiny` (finance), `medqa_tiny` (medical), `iot_tiny` (iot)
- Baseline pipeline verified with `tiny-gpt2` on all three tasks
- CSV schema stable across tasks
- ACE + metrics work delegated to Sathwik and Archit

âš ï¸ **Mac M3 Limitations:**
- Phi-3 Mini and larger models may be slow on Mac M3
- Recommended: Use Mac for tiny models + debugging
- Production runs: Use GPU laptop (Shatvik) or supercomputer (Archit)

---

## For Sathwik (Person 2 â€” ACE / Prompts)

### Your Primary Files

**`slm_ace/ace_roles.py`** â€” This is your main workspace:

1. **`build_generator_prompt()`** (lines ~8-59)
   - Currently has basic prompt structure
   - TODO(Sathwik) markers indicate where to improve
   - **Goal**: Create prompts that effectively incorporate playbook strategies
   - **Test**: Generator prompts should lead to better answers when playbook has good entries

2. **`build_reflector_prompt()`** (lines ~62-123)
   - Basic reflection prompt exists
   - **Goal**: Generate specific, actionable lessons (not generic advice)
   - **Test**: Reflector should produce lessons like "For TAT-QA, always compute total revenue before tax" not "think carefully"

3. **`parse_reflector_output_to_lessons()`** (lines ~126-145)
   - Basic parser (splits on bullets)
   - **Goal**: Handle various output formats robustly
   - **Test**: Should extract clean lessons from model output

4. **`choose_lessons_for_playbook()`** (lines ~148-207)
   - Basic filtering and deduplication
   - **Goal**: Filter out generic/duplicate lessons before adding to playbook
   - **Test**: Only high-quality, unique lessons should enter playbook

**`slm_ace/runner.py`** â€” Secondary adjustments:

- `run_dataset_ace()` (lines ~110-288) is already wired up
- You can adjust:
  - `reflect_on_correct_every_n` parameter (currently 5)
  - `prune_every_n` parameter (currently 10)
  - Reflection triggers (currently: always on incorrect, periodically on correct)

**Important**: You will implement and iterate ACE logic in:
- `slm_ace/ace_roles.py` (main file for prompts)
- `slm_ace/playbook.py` (data structures â€” don't change API)

You do NOT need to touch `model_manager.py` or `runner.py` function signatures.

### Testing Your Changes

**Quick test (tiny model, Mac-friendly):**
```bash
python -m scripts.run_experiment \
  --model-id sshleifer/tiny-gpt2 \
  --task-name tatqa_tiny \
  --mode ace \
  --playbook-path playbooks/tatqa_playbook.jsonl \
  --output-path results/tatqa_tiny_ace_test.csv \
  --limit 3
```

**ACE epoch evolution test (recommended for development):**
```bash
# Test ACE evolution over multiple epochs (tiny model, Mac-friendly)
python -m scripts.run_ace_epoch \
  --model-id sshleifer/tiny-gpt2 \
  --task-name tatqa_tiny \
  --epochs 3 \
  --limit 3 \
  --output-dir results/ace_tiny
```

**Real test (Phi-3 Mini, on GPU laptop):**
```bash
# Test on any of the three tasks: tatqa_tiny, medqa_tiny, iot_tiny
python -m scripts.run_experiment \
  --model-id phi3-mini \
  --task-name tatqa_tiny \
  --mode ace \
  --playbook-path playbooks/tatqa_playbook.jsonl \
  --output-path results/tatqa_phi3_ace.csv

# Or use ACE epoch driver for multi-epoch evolution:
python -m scripts.run_ace_epoch \
  --model-id phi3-mini \
  --task-name tatqa_tiny \
  --epochs 5 \
  --output-dir results/ace_phi3
```

**What to check:**
- Playbook entries are specific and domain-relevant (not generic)
- ACE mode accuracy improves over baseline after several examples
- Playbook grows sensibly (not too fast, not too slow)
- Reflection produces useful lessons

### Success Criteria

âœ… **ACE mode shows improvement:**
- After 10-20 examples, ACE accuracy > baseline accuracy
- Playbook contains 5-15 useful, specific strategies
- Latency increase is acceptable (<2x baseline)

âœ… **Playbook quality:**
- Lessons are specific (e.g., "For finance QA, extract numbers before arithmetic")
- No generic fluff (e.g., "think carefully", "be thorough")
- Deduplication works (similar lessons merge)

---

## For Archit (Person 3 â€” Metrics / Evaluation)

### Your Primary Files

**`slm_ace/metrics.py`** â€” Main metrics implementation:

1. **`compute_accuracy()`** (lines ~8-33)
   - Currently: Simple case-insensitive exact match
   - **Goal**: Robust answer comparison
   - **TODO(Archit)** markers indicate extensions needed:
     - Numeric format normalization ("100" vs "100.0" vs "100.00")
     - Punctuation/whitespace normalization
     - Semantic similarity for partial credit
     - Per-domain metrics (EM for finance, F1 for medical)

2. **`compute_average_latency()`** (lines ~36-44)
   - Currently: Simple average
   - **Goal**: Add percentiles (p50, p95, p99)
   - Consider per-step breakdown (generation vs reflection latency)

**Summary script: `scripts/summarize_results.py`** (already created by Person 1)

This script:
- Reads all CSV files in a directory
- Groups by `model_id`, `task_name`, `mode` (and `epoch` if present)
- Computes aggregate statistics (accuracy, latency, sample counts)
- Outputs CSV summary and Markdown table

**You will implement and extend metrics in:**
- `slm_ace/metrics.py` (main file for answer comparison and latency metrics)

**You will run larger models and more samples on:**
- GPU laptop or supercomputer

**Use these scripts for experiments:**
- `scripts.run_experiment` â€” Single baseline or ACE run
- `scripts.run_ace_epoch` â€” Multi-epoch ACE evolution (epoch 0 = baseline, epochs 1+ = ACE)
- `scripts.summarize_results` â€” Aggregate CSVs into summary tables

You should track per-epoch improvements and possibly add CI/bootstrap scripts later.

### Testing Your Changes

**Generate test data:**
```bash
# Baseline run
python -m scripts.run_experiment \
  --model-id phi3-mini \
  --task-name tatqa_tiny \
  --mode baseline \
  --output-path results/tatqa_phi3_baseline.csv

# ACE run
python -m scripts.run_experiment \
  --model-id phi3-mini \
  --task-name tatqa_tiny \
  --mode ace \
  --playbook-path playbooks/tatqa_playbook.jsonl \
  --output-path results/tatqa_phi3_ace.csv
```

**Then analyze:**
```bash
python scripts/summarize_results.py \
  --baseline results/tatqa_phi3_baseline.csv \
  --ace results/tatqa_phi3_ace.csv \
  --output results/comparison.md
```

### Success Criteria

âœ… **Robust metrics:**
- Handles numeric answers correctly (100 = 100.0 = 100.00)
- Handles text answers with minor variations
- Per-domain metrics work (EM for finance, F1 for medical if needed)

âœ… **Summary tables:**
- Clear comparison: baseline vs ACE accuracy
- Latency breakdown (generation vs reflection)
- Per-model performance table
- Ready for paper Methods/Results sections

---

## Common Workflow

### 1. Person 1 (Suryodaya) runs smoke test
```bash
python -m scripts.run_experiment \
  --model-id sshleifer/tiny-gpt2 \
  --task-name tatqa_tiny \
  --mode baseline \
  --output-path results/smoke_test.csv
```

### 2. Sathwik develops ACE prompts
- Edit `slm_ace/ace_roles.py`
- Test with tiny model first
- Then test with Phi-3 Mini on GPU laptop

### 3. Archit develops metrics
- Edit `slm_ace/metrics.py`
- Create `scripts/summarize_results.py`
- Analyze results from Sathwik's runs

### 4. Integration test
- All three run full pipeline
- Compare results
- Document findings

---

## File Ownership

**Person 1 (Suryodaya) â€” Infrastructure:**
- `slm_ace/config.py` âœ…
- `slm_ace/model_manager.py` âœ…
- `slm_ace/utils.py` âœ…
- `slm_ace/runner.py` (pipeline, but Sathwik adjusts ACE logic)
- `scripts/run_experiment.py` âœ…
- `DEV_NOTES_PERSON1.md` âœ…

**Person 2 (Sathwik) â€” ACE Logic:**
- `slm_ace/ace_roles.py` ðŸ”§ (your main file)
- `slm_ace/runner.py` (adjust ACE parameters)

**Person 3 (Archit) â€” Metrics:**
- `slm_ace/metrics.py` ðŸ”§ (your main file)
- `scripts/summarize_results.py` ðŸ”§ (create this)

**Shared:**
- `slm_ace/playbook.py` (data structures â€” don't change API)
- `data/tasks/*.json` (add more datasets as needed)
- `tests/` (add tests for your changes)

---

## Quick Reference Commands

### Smoke test (Mac M3)
```bash
python -m scripts.run_experiment \
  --model-id sshleifer/tiny-gpt2 \
  --task-name tatqa_tiny \
  --mode baseline \
  --output-path results/smoke_test.csv
```

### Real baseline (GPU laptop / supercomputer)
```bash
python -m scripts.run_experiment \
  --model-id phi3-mini \
  --task-name tatqa_tiny \
  --mode baseline \
  --output-path results/tatqa_phi3_baseline.csv
```

### Real ACE (GPU laptop / supercomputer)
```bash
python -m scripts.run_experiment \
  --model-id phi3-mini \
  --task-name tatqa_tiny \
  --mode ace \
  --playbook-path playbooks/tatqa_playbook.jsonl \
  --output-path results/tatqa_phi3_ace.csv
```

### Quick test (limit examples)
```bash
python -m scripts.run_experiment \
  --model-id phi3-mini \
  --task-name tatqa_tiny \
  --mode ace \
  --playbook-path playbooks/tatqa_playbook.jsonl \
  --output-path results/test.csv \
  --limit 3
```

---

## Questions?

- **Infrastructure issues**: Ask Suryodaya
- **ACE/prompt questions**: Ask Sathwik
- **Metrics/evaluation questions**: Ask Archit
- **General project questions**: Group discussion

