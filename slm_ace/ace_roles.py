"""ACE-style roles: Generator, Reflector, and Curator prompt builders."""

from typing import List, Optional

from slm_ace.playbook import Playbook


def build_generator_prompt(
    domain: str,
    playbook: Playbook,
    question: str,
    context: Optional[str] = None,
    ace_mode: str = "ace_full",
    token_budget: int = 500,
    current_step: int = 0,
) -> str:
    """
    Build a prompt for the Generator role (model that answers questions).
    
    This prompt includes:
    - Domain context
    - Top-k playbook strategies for the domain (or budget-limited for working memory)
    - The question and optional context
    
    Args:
        domain: Domain name (e.g., "finance", "medical").
        playbook: The ACE playbook.
        question: The question to answer.
        context: Optional context/background information.
        ace_mode: ACE mode ("ace_full" or "ace_working_memory").
        token_budget: Token budget for working memory mode (default: 500).
        current_step: Current step counter for recency calculation.
        
    Returns:
        Formatted prompt string.
    """
    # Get strategies from playbook based on mode
    if ace_mode == "ace_working_memory":
        # Use token-budgeted selection for working memory mode
        from slm_ace.config import ACE_MODE_WORKING
        top_strategies = playbook.get_top_entries_for_budget(
            domain=domain,
            token_budget=token_budget,
            current_step=current_step,
        )
    else:
        # Use top-k for full ACE mode
        top_strategies = playbook.get_top_k(domain, k=5, current_step=current_step)
    
    # Build domain header
    domain_header = f"You are an expert assistant specializing in {domain} domain questions."
    
    # Build playbook section
    if top_strategies:
        playbook_section = "\n\nRelevant strategies from previous experience:\n"
        for i, strategy in enumerate(top_strategies, 1):
            playbook_section += f"{i}. {strategy.text}\n"
    else:
        playbook_section = "\n\n(No prior strategies available yet.)\n"
    
    # Build question section
    question_section = f"\n\nQuestion: {question}\n"
    
    if context:
        question_section = f"\n\nContext: {context}\n{question_section}"
    
    # TODO(Shatvik): Refine generator prompt wording for {domain}
    # - Consider adding more structured formatting (e.g., few-shot examples)
    # - Add instructions for reasoning/explanation if needed
    # - Domain-specific prompt engineering (finance vs medical vs IoT)
    # - Experiment with different playbook integration strategies
    
    prompt = f"{domain_header}{playbook_section}{question_section}\nAnswer:"
    
    return prompt


def build_reflector_prompt(
    domain: str,
    question: str,
    context: Optional[str],
    model_answer: str,
    ground_truth: str,
    reasoning: Optional[str] = None,
) -> str:
    """
    Build a prompt for the Reflector role (model that generates lessons).
    
    The Reflector analyzes the model's answer against ground truth and produces
    specific, actionable lessons.
    
    Args:
        domain: Domain name.
        question: The original question.
        context: Optional context that was provided.
        model_answer: The answer generated by the model.
        ground_truth: The correct answer.
        reasoning: Optional reasoning from the model.
        
    Returns:
        Formatted prompt string.
    """
    correct = model_answer.strip().lower() == ground_truth.strip().lower()
    status = "correct" if correct else "incorrect"
    
    prompt = f"""You are analyzing a {domain} domain question-answer pair.

Question: {question}
"""
    
    if context:
        prompt += f"Context: {context}\n"
    
    prompt += f"""
Model Answer: {model_answer}
Correct Answer: {ground_truth}
Status: {status}
"""
    
    if reasoning:
        prompt += f"Model Reasoning: {reasoning}\n"
    
    prompt += """
Please generate 1-3 short, specific bullet-point lessons that:
- Are actionable and domain-specific (avoid generic advice like "think carefully")
- Highlight what went wrong (if incorrect) or what strategy worked (if correct)
- Can be used as a strategy for future similar questions

Format your response as bullet points, one per line, starting with "-" or "•".
"""
    
    # TODO(Shatvik): Refine reflector prompt design
    # - Consider adding examples of good vs bad lessons in the prompt
    # - Add domain-specific reflection guidelines
    # - Consider multi-turn reflection for complex errors
    # - Experiment with different lesson formats (structured vs free-form)
    # - Add constraints to avoid generic advice (e.g., "think carefully")
    
    return prompt


def parse_reflector_output_to_lessons(text: str) -> List[str]:
    """
    Parse the Reflector's output into a list of lesson strings.
    
    Args:
        text: Raw output from the Reflector model.
        
    Returns:
        List of lesson strings (cleaned and filtered).
    """
    lessons = []
    lines = text.strip().split("\n")
    
    for line in lines:
        line = line.strip()
        # Look for bullet points
        if line.startswith("-") or line.startswith("•"):
            # Remove bullet marker and clean
            lesson = line.lstrip("-•").strip()
            if lesson:
                lessons.append(lesson)
        elif line and len(line) > 10:  # Also accept non-bullet lines if substantial
            lessons.append(line)
    
    return lessons


def choose_lessons_for_playbook(
    domain: str,
    lessons: List[str],
    existing_playbook: Playbook,
    min_length: int = 15,
) -> List[str]:
    """
    Filter and deduplicate lessons before adding to playbook.
    
    Args:
        domain: Domain name.
        lessons: List of candidate lesson strings.
        existing_playbook: Current playbook to check against.
        min_length: Minimum character length for a lesson to be kept.
        
    Returns:
        Filtered list of lessons suitable for playbook.
    """
    filtered = []
    
    # Generic phrases to filter out
    generic_phrases = [
        "think carefully",
        "be careful",
        "pay attention",
        "consider",
        "remember",
        "make sure",
    ]
    
    for lesson in lessons:
        lesson = lesson.strip()
        
        # Filter too short
        if len(lesson) < min_length:
            continue
        
        # Filter generic
        lesson_lower = lesson.lower()
        if any(phrase in lesson_lower for phrase in generic_phrases):
            # Only skip if it's mostly generic
            if len(lesson.split()) < 5:
                continue
        
        # Check against existing entries (simple deduplication)
        is_duplicate = False
        for entry in existing_playbook.entries:
            if entry.domain == domain:
                if lesson.lower() in entry.text.lower() or entry.text.lower() in lesson.lower():
                    is_duplicate = True
                    break
        
        if not is_duplicate:
            filtered.append(lesson)
    
    return filtered


def build_self_refine_critique_prompt(
    domain: str,
    question: str,
    context: Optional[str],
    initial_answer: str,
    ground_truth: str,
) -> str:
    """
    Build a prompt for the model to critique its own answer (for self_refine mode).
    
    Args:
        domain: Domain name.
        question: The original question.
        context: Optional context that was provided.
        initial_answer: The initial answer generated by the model.
        ground_truth: The correct answer.
        
    Returns:
        Formatted prompt string for critique.
    """
    prompt = f"""You are analyzing a {domain} domain question-answer pair.

Question: {question}
"""
    
    if context:
        prompt += f"Context: {context}\n"
    
    prompt += f"""
Your Initial Answer: {initial_answer}
Correct Answer: {ground_truth}

Please provide a brief critique of your initial answer. What went wrong or what could be improved?
Focus on specific issues, not generic advice.
"""
    
    return prompt


def build_self_refine_rewrite_prompt(
    domain: str,
    question: str,
    context: Optional[str],
    initial_answer: str,
    critique: str,
    ground_truth: str,
) -> str:
    """
    Build a prompt for the model to rewrite its answer after seeing critique (for self_refine mode).
    
    Args:
        domain: Domain name.
        question: The original question.
        context: Optional context that was provided.
        initial_answer: The initial answer generated by the model.
        critique: The critique of the initial answer.
        ground_truth: The correct answer.
        
    Returns:
        Formatted prompt string for rewriting.
    """
    prompt = f"""You are answering a {domain} domain question. You previously gave an answer, received critique, and now see the correct answer.

Question: {question}
"""
    
    if context:
        prompt += f"Context: {context}\n"
    
    prompt += f"""
Your Initial Answer: {initial_answer}
Critique: {critique}
Correct Answer: {ground_truth}

Please provide your final, improved answer based on the critique and the correct answer.
"""
    
    return prompt

